{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifier_check.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQwXeC3E+o4dAOlEBPP/Gi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowun/BladderCancer_AMC/blob/master/Notebooks/classifier_check_20211001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKSS55r40Z9C"
      },
      "source": [
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/gdrive')\n",
        "home_path = '/content/gdrive/My Drive/BladderCancer_AMC/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrrU5_-ObtcO"
      },
      "source": [
        "!pip install lifelines\n",
        "!pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIXTzaW401Et"
      },
      "source": [
        "!git clone https://github.com/gowun/BladderCancer_AMC.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm7bPLhc013x"
      },
      "source": [
        "from BladderCancer_AMC.ModelingTools import utils as ut\n",
        "from BladderCancer_AMC.ModelingTools import clustering as cl\n",
        "from BladderCancer_AMC.ModelingTools import tree_modeling as tm\n",
        "from BladderCancer_AMC.ModelingTools import linear_modeling as lm\n",
        "from BladderCancer_AMC.ModelingTools import figure as fe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7IP10uG1WHM"
      },
      "source": [
        "data_labels = ['MDA_MVAC', 'MDA_DDMVAC', 'Meta_Datasets', 'AMC']\n",
        "classifiers = ut.load_data(home_path + 'intersect_classifiers.pkl', 'pickle')\n",
        "datasets = ut.load_data(f'{home_path}scaled_datasets_3mths_20211001.pkl', 'pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRNnunrUYY8A"
      },
      "source": [
        "def random_oversampling(idx_list, n):\n",
        "  np.random.seed(1234)\n",
        "  return np.random.choice(idx_list, n)\n",
        "\n",
        "MAX_ROW = 100000\n",
        "over_idx_dict = dict()\n",
        "for i, d in enumerate(datasets['power']):\n",
        "  over_idx_dict[data_labels[i]] = random_oversampling(list(range(len(d))), MAX_ROW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgx_J4G4ZpqR"
      },
      "source": [
        "%%time\n",
        "topK = 10\n",
        "### Simple Random Forest for Variable Selection when the classifier inclued 10 more variables \n",
        "### And.. select the best normalizer\n",
        "methods = ['power', 'standard', 'rankgauss']\n",
        "sum_score = dict()\n",
        "columns = dict()\n",
        "for m in methods:\n",
        "  sum_score[m] = 0\n",
        "  columns[m] = dict()\n",
        "  for cls, vars in classifiers.items():\n",
        "    if len(vars) > topK:\n",
        "      for i, l in enumerate(data_labels):\n",
        "        X, y = datasets[m][i][vars].iloc[over_idx_dict[l]], np.array(datasets[m][i]['response'])[over_idx_dict[l]]\n",
        "        sample_leaf = round(MAX_ROW / len(datasets[m][i]) * 3/2)\n",
        "        result = tm.random_forest_with_performance([X, y], 50, 3, sample_leaf)\n",
        "        sum_score[m] = sum_score[m] + result['performance']['AUC'] + result['performance']['PRAUC'] - abs(result['performance']['R2'])\n",
        "        columns[m]['_'.join([cls, l])] = result['feature importance']['feature'].values[:topK]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNSj-l1-nZt8"
      },
      "source": [
        "sum_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ0ivSOKdPss"
      },
      "source": [
        "#BEST_NOR = methods[np.argmax(list(sum_score.values()))]\n",
        "#print(BEST_NOR)\n",
        "BEST_NOR = 'standard'\n",
        "final_columns = {}\n",
        "for cls, vars in classifiers.items():\n",
        "  if len(vars) > 10:\n",
        "    names = list(filter(lambda x: x.startswith(cls), columns[BEST_NOR].keys()))\n",
        "    tmp = list()\n",
        "    for n in names:\n",
        "      tmp += list(columns[BEST_NOR][n])\n",
        "    final_columns[cls] = sorted(set(tmp))\n",
        "  else:\n",
        "    final_columns[cls] = sorted(vars)\n",
        "final_csv = dict()\n",
        "max_len = max(list(map(lambda x: len(x), final_columns.values())))\n",
        "for cls, vars in final_columns.items():\n",
        "  print(cls, len(classifiers[cls]), len(vars))\n",
        "  ll = max_len - len(vars)\n",
        "  final_csv[cls] = list(vars) + [''] * ll\n",
        "ut.save_data(pd.DataFrame(final_csv), home_path + 'final_classifiers.csv', 'csv')\n",
        "\n",
        "topK_columns = {}\n",
        "for l in data_labels:\n",
        "  print(l)\n",
        "  topK_columns[l] = {}\n",
        "  for cls, vars in classifiers.items():\n",
        "    if len(vars) <= 10:\n",
        "      topK_columns[l][cls] = sorted(vars)\n",
        "    else:\n",
        "      topK_columns[l][cls] = columns[BEST_NOR][cls + '_' + l]\n",
        "  topK_csv = {}\n",
        "  max_len = max(list(map(lambda x: len(x), topK_columns[l].values())))\n",
        "  for cls, vars in topK_columns[l].items():\n",
        "    ll = max_len - len(vars)\n",
        "    topK_csv[cls] = list(vars) + [''] * ll\n",
        "  ut.save_data(pd.DataFrame(topK_csv), home_path + f'top10_{l}_classifiers.csv', 'csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGU4wXyQrBIk"
      },
      "source": [
        "from itertools import permutations\n",
        "orders = list(range(len(data_labels)))\n",
        "orders = list(permutations(orders, 2))\n",
        "orders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYh6HPwisAJX"
      },
      "source": [
        "from scipy.stats import ttest_ind\n",
        "from itertools import chain\n",
        "\n",
        "def confirm_by_ttest(arr1, arr2, pvalue=0.05):\n",
        "  tmp = ttest_ind(arr1, arr2, equal_var=False)\n",
        "  if tmp.pvalue <= pvalue:\n",
        "    differ = True\n",
        "  else:\n",
        "    differ = False\n",
        "  return differ, tmp\n",
        "\n",
        "def modeling_with_various_features(X_tr, y_tr, X_val, y_val, fts_dict, md_mode, Xy_ts_sets=None):\n",
        "  result = dict()\n",
        "  result['models'] = dict()\n",
        "  result['scores_tr_val'] = dict()\n",
        "  result['ttest_vals'] = dict()\n",
        "  result['best_classifiers'] = []\n",
        "\n",
        "  perf = []\n",
        "  metrics = ['AUC', 'PRAUC', 'R2']\n",
        "  for k, filtered in fts_dict.items():\n",
        "\n",
        "    if md_mode == 'logistic':\n",
        "      tmp = lm.logiReg_model_with_performance([X_tr[filtered], y_tr], 10, class_weight='balanced')\n",
        "    elif md_mode == 'decision':\n",
        "      tmp = tm.tree_model_with_performance([X_tr[filtered], y_tr], 3, 3, class_weight='balanced')\n",
        "    elif md_mode == 'random':\n",
        "      tmp = tm.random_forest_with_performance([X_tr[filtered], y_tr], 50, 3, 3)\n",
        "    \n",
        "    prob_tr = tmp['model'].predict_proba(X_tr[filtered])[:, 1]\n",
        "    pred_val = tmp['model'].predict(X_val[filtered])\n",
        "    prob_val = tmp['model'].predict_proba(X_val[filtered])[:, 1]\n",
        "\n",
        "    result['scores_tr_val'][k] = [prob_tr, prob_val]\n",
        "\n",
        "    pred_ts_sets = []\n",
        "    prob_ts_sets = []\n",
        "    if Xy_ts_sets is not None:\n",
        "      for X_ts, y_ts in Xy_ts_sets:\n",
        "        pred_ts_sets.append(tmp['model'].predict(X_ts[filtered]))\n",
        "        prob_ts_sets.append(tmp['model'].predict_proba(X_ts[filtered])[:, 1])\n",
        "        result['scores_tr_val'][k] += [prob_ts_sets[-1]]\n",
        "    \n",
        "    ### 스코어 검증\n",
        "    div_tr = []\n",
        "    div_val = []\n",
        "    for i in range(2):\n",
        "      div_tr.append(prob_tr[np.array(y_tr) == i])\n",
        "      div_val.append(prob_val[np.array(y_val) == i])\n",
        "    # 1. 동일데이터 내 R vs. NR 차이가 유효한가\n",
        "    # 2. R 끼리 유사한가\n",
        "    # 3. NR 끼리 유사한가\n",
        "    result['ttest_vals'][k] = [confirm_by_ttest(div_tr[0], div_tr[1])[0], confirm_by_ttest(div_val[0], div_val[1])[0], not confirm_by_ttest(div_tr[0], div_val[0])[0], not confirm_by_ttest(div_tr[1], div_val[1])[0]]\n",
        "    if sum(result['ttest_vals'][k]) == 4:\n",
        "      result['best_classifiers'].append(k)\n",
        "    result['models'][k] = tmp\n",
        "\n",
        "    val = tm.compute_performance(y_val, pred_val, prob_val)\n",
        "    ts_sets = []\n",
        "    if Xy_ts_sets is not None:\n",
        "      for i, (X_ts, y_ts) in enumerate(Xy_ts_sets):\n",
        "        ts_sets.append(tm.compute_performance(y_ts, pred_ts_sets[i], prob_ts_sets[i]))\n",
        "    tmp_ = []\n",
        "    for met in metrics:\n",
        "      tmp_ += [tmp['performance'][met], val[met]] + list(map(lambda x: x[met], ts_sets))\n",
        "    perf.append(tmp_)\n",
        "    print(k, tmp_)\n",
        "\n",
        "  r_perf = []\n",
        "  if len(result['best_classifiers']) > 0:\n",
        "    for c in result['best_classifiers']:\n",
        "      ii = list(fts_dict.keys()).index(c)\n",
        "      r_perf.append(perf[ii])\n",
        "    cols_ = list(map(lambda x: 'ts'+str(x), range(len(ts_sets))))\n",
        "    cols = list(chain(*map(lambda x: list(map(lambda y: y + '_' + x, ['tr', 'val'] + cols_)), metrics)))\n",
        "    comp = pd.DataFrame(r_perf, columns=cols, index=result['best_classifiers'])\n",
        "    print(comp)\n",
        "  else:\n",
        "    comp = None\n",
        "  return result, comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMqfiV9kolN3"
      },
      "source": [
        "def validated_models(orders, datasets, best_nor, col_dict, data_labels, over_idx_dict, mode):\n",
        "  mode_al, mode_cls = mode.split('_')\n",
        "  models = dict()\n",
        "  for o1, o2 in orders:\n",
        "    X_tr, y_tr = datasets[best_nor][o1].iloc[over_idx_dict[data_labels[o1]]], np.array(datasets[best_nor][o1]['response'])[over_idx_dict[data_labels[o1]]]\n",
        "    X_ts, y_ts = datasets[best_nor][o2], np.array(datasets[best_nor][o2]['response'])\n",
        "    ts_idxs = sorted({0, 1, 2, 3} - {o1, o2})\n",
        "    Xy_ts_sets = []\n",
        "    for i in ts_idxs:\n",
        "      Xy_ts_sets.append([datasets[best_nor][i], np.array(datasets[best_nor][i]['response'])])\n",
        "    if mode_cls == 'union':\n",
        "      cols = col_dict\n",
        "    elif mode_cls == 'topK':\n",
        "      cols = col_dict[data_labels[o1]]\n",
        "    total = modeling_with_various_features(X_tr, y_tr, X_ts, y_ts, cols, mode_al, Xy_ts_sets)\n",
        "    print(o1, o2, len(total[0]['best_classifiers']), ts_idxs)\n",
        "    if len(total[0]['best_classifiers']) > 0:\n",
        "      models['->'.join([data_labels[o1], data_labels[o2]])] = total\n",
        "\n",
        "  if len(models) > 0:\n",
        "    ut.save_data(models, home_path + f'{mode_cls}_{mode_al}.pkl', 'pkl')\n",
        "    tmp = []\n",
        "    for key in models.keys():\n",
        "      tt = models[key][1].copy()\n",
        "      tt.index = list(map(lambda x: key + ',' + x, tt.index))\n",
        "      tmp.append(tt)\n",
        "    comp = pd.concat(tmp)\n",
        "    print(comp)\n",
        "    return models, comp\n",
        "  else:\n",
        "    return None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ6ub-P7qtXk"
      },
      "source": [
        "%%time\n",
        "log_union, log_union_comp = validated_models(orders, datasets, BEST_NOR, final_columns, data_labels, over_idx_dict, 'logistic_union')\n",
        "log_topK, log_topK_comp = validated_models(orders, datasets, BEST_NOR, topK_columns, data_labels, over_idx_dict, 'logistic_topK')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDqH10TNHh4b"
      },
      "source": [
        "ut.save_data(over_idx_dict, home_path + f'over_idx_dict_20211001.pkl', 'pickle')\n",
        "ut.save_data([log_union, log_union_comp], home_path + f'log_union_info_20211001.pkl', 'pickle')\n",
        "ut.save_data([log_topK, log_topK_comp], home_path + f'log_topK_info_20211001.pkl', 'pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3peCs1Jw9bT5"
      },
      "source": [
        "log_union_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd7PllOu-MXn"
      },
      "source": [
        "log_topK_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5VHzacs9nWu"
      },
      "source": [
        "%%time\n",
        "dt_union, dt_union_comp = validated_models(orders, datasets, BEST_NOR, final_columns, data_labels, over_idx_dict, 'decision_union')\n",
        "dt_topK, dt_topK_comp = validated_models(orders, datasets, BEST_NOR, topK_columns, data_labels, over_idx_dict, 'decision_topK')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7co5aLEL9ncr"
      },
      "source": [
        "dt_union_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P0D43HWGMWz"
      },
      "source": [
        "dt_topK_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIWUbggHtdwJ"
      },
      "source": [
        "%%time\n",
        "rf_union, rf_union_comp = validated_models(orders, datasets, BEST_NOR, final_columns, data_labels, over_idx_dict, 'random_union')\n",
        "rf_topK, rf_topK_comp = validated_models(orders, datasets, BEST_NOR, topK_columns, data_labels, over_idx_dict, 'random_topK')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLa5RTL_td2U"
      },
      "source": [
        "rf_union_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D_F9Xsrtd6x"
      },
      "source": [
        "rf_topK_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF6HZWytdhKA"
      },
      "source": [
        "def draw_box_plots(tag, cls, model, cols, datasets, data_labels):\n",
        "  tags = tag.split('->')\n",
        "  tags = [tags[0]] + tags[1].split(',')\n",
        "  idxs = list(map(lambda x: data_labels.index(x), tags[:2]))\n",
        "  r_idxs = sorted({0, 1, 2, 3} - set(idxs))\n",
        "  ll = ['M', 'V', 'T0', 'T1']\n",
        "\n",
        "  scores = []\n",
        "  labels = []\n",
        "  for i, j in enumerate(idxs + r_idxs):\n",
        "    yy = datasets[j]['response']\n",
        "    scores.append(model.predict_proba(datasets[j][cols])[:, 1])\n",
        "    tmp = np.array([f'{i}.{ll[i]}_{data_labels[j]}_NR'] * len(yy))\n",
        "    tmp[np.array(yy) == 1.0] = f'{i}.{ll[i]}_{data_labels[j]}_R'\n",
        "    labels.append(tmp)\n",
        "  \n",
        "  fe.plot_box(np.concatenate(scores), 'y', np.concatenate(labels), tag + ' + ' + cls)\n",
        "\n",
        "def draw_all(models, datasets, data_labels):\n",
        "  for i in models.keys():\n",
        "    for j in models[i][0]['best_classifiers']:\n",
        "      draw_box_plots(i, j, models[i][0]['models'][j]['model'], list(models[i][0]['models'][j]['columns']), datasets, data_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryUACx5OIJmj"
      },
      "source": [
        "draw_all(log_union, datasets[BEST_NOR], data_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVORUPT8kWtK"
      },
      "source": [
        "draw_all(log_topK, datasets[BEST_NOR], data_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4gA2YjHL0zz"
      },
      "source": [
        "draw_all(dt_union, datasets[BEST_NOR], data_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33IKA-C3MBM1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}