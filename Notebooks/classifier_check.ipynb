{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifier_check.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvsNfOxyX6on8k3kmnesPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowun/BladderCancer_AMC/blob/master/Notebooks/classifier_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKSS55r40Z9C"
      },
      "source": [
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/gdrive')\n",
        "home_path = '/content/gdrive/My Drive/BladderCancer_AMC/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrrU5_-ObtcO"
      },
      "source": [
        "!pip install lifelines\n",
        "!pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIXTzaW401Et"
      },
      "source": [
        "!git clone https://github.com/gowun/BladderCancer_AMC.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm7bPLhc013x"
      },
      "source": [
        "from BladderCancer_AMC.ModelingTools import utils as ut\n",
        "from BladderCancer_AMC.ModelingTools import clustering as cl\n",
        "from BladderCancer_AMC.ModelingTools import tree_modeling as tm\n",
        "from BladderCancer_AMC.ModelingTools import linear_modeling as lm\n",
        "from BladderCancer_AMC.ModelingTools import figure as fe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7IP10uG1WHM"
      },
      "source": [
        "data_labels = ['MDA_MVAC', 'MDA_DDMVAC', 'Meta_Datasets', 'AMC']\n",
        "classifiers = ut.load_data(home_path + 'intersect_classifiers.pkl', 'pickle')\n",
        "datasets = ut.load_data(f'{home_path}scaled_datasets_3mths.pkl', 'pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRNnunrUYY8A"
      },
      "source": [
        "def random_oversampling(idx_list, n):\n",
        "  np.random.seed(1234)\n",
        "  return np.random.choice(idx_list, n)\n",
        "\n",
        "MAX_ROW = 100000\n",
        "over_idx_dict = dict()\n",
        "for i, d in enumerate(datasets['power']):\n",
        "  over_idx_dict[data_labels[i]] = random_oversampling(list(range(len(d))), MAX_ROW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgx_J4G4ZpqR"
      },
      "source": [
        "%%time\n",
        "topK = 10\n",
        "### Simple Random Forest for Variable Selection when the classifier inclued 10 more variables \n",
        "### And.. select the best normalizer\n",
        "methods = ['power', 'standard', 'rankgauss']\n",
        "sum_score = dict()\n",
        "columns = dict()\n",
        "for m in methods:\n",
        "  sum_score[m] = 0\n",
        "  columns[m] = dict()\n",
        "  for cls, vars in classifiers.items():\n",
        "    if len(vars) > topK:\n",
        "      for i, l in enumerate(data_labels):\n",
        "        X, y = datasets[m][i][vars].iloc[over_idx_dict[l]], np.array(datasets[m][i]['response'])[over_idx_dict[l]]\n",
        "        sample_leaf = round(MAX_ROW / len(datasets[m][i]) * 3/2)\n",
        "        result = tm.random_forest_with_performance([X, y], 50, 3, sample_leaf)\n",
        "        sum_score[m] += sum(result['performance'].values())\n",
        "        columns[m]['_'.join([cls, l])] = result['feature importance']['feature'].values[:topK]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNSj-l1-nZt8"
      },
      "source": [
        "sum_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ0ivSOKdPss"
      },
      "source": [
        "BEST_NOR = methods[np.argmax(list(sum_score.values()))]\n",
        "print(BEST_NOR)\n",
        "final_columns = classifiers.copy()\n",
        "for cls, vars in classifiers.items():\n",
        "  if len(vars) > 10:\n",
        "    names = list(filter(lambda x: x.startswith(cls), columns[BEST_NOR].keys()))\n",
        "    tmp = list()\n",
        "    for n in names:\n",
        "      tmp += list(columns[BEST_NOR][n])\n",
        "    final_columns[cls] = list(set(tmp))\n",
        "max_len = max(list(map(lambda x: len(x), final_columns.values())))\n",
        "final_csv = dict()\n",
        "for cls, vars in final_columns.items():\n",
        "  print(cls, len(classifiers[cls]), len(vars))\n",
        "  ll = max_len - len(vars)\n",
        "  final_csv[cls] = list(vars) + [''] * ll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMCoVoOxRCIf"
      },
      "source": [
        "ut.save_data(pd.DataFrame(final_csv), home_path + 'final_classifiers.csv', 'csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGU4wXyQrBIk"
      },
      "source": [
        "from itertools import permutations\n",
        "orders = list(range(len(data_labels)))\n",
        "orders = list(permutations(orders, 2))\n",
        "orders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYh6HPwisAJX"
      },
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def confirm_by_ttest(arr1, arr2, pvalue=0.05):\n",
        "  tmp = ttest_ind(arr1, arr2, equal_var=False)\n",
        "  if tmp.pvalue <= pvalue:\n",
        "    differ = True\n",
        "  else:\n",
        "    differ = False\n",
        "  return differ, tmp\n",
        "\n",
        "def modeling_with_various_features(X_tr, y_tr, X_ts, y_ts, fts_dict, md_mode):\n",
        "  result = dict()\n",
        "  result['models'] = dict()\n",
        "  result['scores_tr_val'] = dict()\n",
        "  result['ttest_vals'] = dict()\n",
        "  result['best_classifiers'] = []\n",
        "\n",
        "  perf_tr = []\n",
        "  perf_ts = []\n",
        "  for k, filtered in fts_dict.items():\n",
        "\n",
        "    if md_mode == 'logistic':\n",
        "      tmp = lm.logiReg_model_with_performance([X_tr[filtered], y_tr], 10, class_weight='balanced')\n",
        "    elif md_mode == 'decision':\n",
        "      tmp = tm.tree_model_with_performance([X_tr[filtered], y_tr], 3, 3, class_weight='balanced')\n",
        "    elif md_mode == 'random':\n",
        "      tmp = tm.random_forest_with_performance([X_tr[filtered], y_tr], 50, 3, 3)\n",
        "    \n",
        "    prob_tr = tmp['model'].predict_proba(X_tr[filtered])[:, 1]\n",
        "    pred_ts = tmp['model'].predict(X_ts[filtered])\n",
        "    prob_ts = tmp['model'].predict_proba(X_ts[filtered])[:, 1]\n",
        "    \n",
        "    result['scores_tr_val'][k] = [prob_tr, prob_ts]\n",
        "    \n",
        "    ### 스코어 검증\n",
        "    div_tr = []\n",
        "    div_ts = []\n",
        "    for i in range(2):\n",
        "      div_tr.append(prob_tr[np.array(y_tr) == i])\n",
        "      div_ts.append(prob_ts[np.array(y_ts) == i])\n",
        "    # 1. 동일데이터 내 R vs. NR 차이가 유효한가\n",
        "    # 2. R 끼리 유사한가\n",
        "    # 3. NR 끼리 유사한가\n",
        "    result['ttest_vals'][k] = [confirm_by_ttest(div_tr[0], div_tr[1])[0], confirm_by_ttest(div_ts[0], div_ts[1])[0], not confirm_by_ttest(div_tr[0], div_ts[0])[0], not confirm_by_ttest(div_tr[1], div_ts[1])[0]]\n",
        "    if sum(result['ttest_vals'][k]) == 4:\n",
        "      result['best_classifiers'].append(k)\n",
        "\n",
        "    result['models'][k] = tmp\n",
        "    perf_tr.append(tmp['performance'])\n",
        "    perf_ts.append(tm.compute_performance(y_ts, pred_ts, prob_ts))\n",
        "    print(k)\n",
        "\n",
        "  if len(result['best_classifiers']) > 0:\n",
        "    r_perf_tr = []\n",
        "    r_perf_ts = []\n",
        "    for c in result['best_classifiers']:\n",
        "      ii = list(fts_dict.keys()).index(c)\n",
        "      r_perf_tr.append(perf_tr[ii])\n",
        "      r_perf_ts.append(perf_ts[ii])\n",
        "    compare_tr = pd.DataFrame(r_perf_tr, index=result['best_classifiers'])\n",
        "    compare_ts = pd.DataFrame(r_perf_ts, index=result['best_classifiers'])\n",
        "    comp = pd.concat([compare_tr, compare_ts], 1)\n",
        "    print(comp)\n",
        "  else:\n",
        "    comp = None\n",
        "  return result, comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24XbfDrcsAMe"
      },
      "source": [
        "%%time\n",
        "## logistic\n",
        "logistic = dict()\n",
        "for o1, o2 in orders:\n",
        "  X_tr, y_tr = datasets[BEST_NOR][o1].iloc[over_idx_dict[data_labels[o1]]], np.array(datasets[BEST_NOR][o1]['response'])[over_idx_dict[data_labels[o1]]]\n",
        "  X_ts, y_ts = datasets[BEST_NOR][o2], np.array(datasets[BEST_NOR][o2]['response'])\n",
        "  total = modeling_with_various_features(X_tr, y_tr, X_ts, y_ts, final_columns, 'logistic')\n",
        "  print(o1, o2)\n",
        "  if len(total[0]['best_classifiers']) > 0:\n",
        "    logistic['->'.join([data_labels[o1], data_labels[o2]])] = total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX0CJzpY9Yeu"
      },
      "source": [
        "logistic.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3peCs1Jw9bT5"
      },
      "source": [
        "logistic['Meta_Datasets->MDA_MVAC'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd7PllOu-MXn"
      },
      "source": [
        "logistic['Meta_Datasets->MDA_DDMVAC'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orxDBtgs-SBp"
      },
      "source": [
        "logistic['Meta_Datasets->AMC'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O3-NoM9sAQD"
      },
      "source": [
        "%%time\n",
        "## decision tree\n",
        "dt = dict()\n",
        "for o1, o2 in orders:\n",
        "  X_tr, y_tr = datasets[BEST_NOR][o1].iloc[over_idx_dict[data_labels[o1]]], np.array(datasets[BEST_NOR][o1]['response'])[over_idx_dict[data_labels[o1]]]\n",
        "  X_ts, y_ts = datasets[BEST_NOR][o2], np.array(datasets[BEST_NOR][o2]['response'])\n",
        "  total = modeling_with_various_features(X_tr, y_tr, X_ts, y_ts, final_columns, 'decision')\n",
        "  if len(total[0]['best_classifiers']) > 0:\n",
        "    dt['->'.join([data_labels[o1], data_labels[o2]])] = total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG5MMpbq7pji"
      },
      "source": [
        "dt.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13EH7PO4sATl"
      },
      "source": [
        "dt['MDA_MVAC->AMC'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSqfVKUO9nTP"
      },
      "source": [
        "%%time\n",
        "## random forest\n",
        "rf = dict()\n",
        "for o1, o2 in orders:\n",
        "  X_tr, y_tr = datasets[BEST_NOR][o1].iloc[over_idx_dict[data_labels[o1]]], np.array(datasets[BEST_NOR][o1]['response'])[over_idx_dict[data_labels[o1]]]\n",
        "  X_ts, y_ts = datasets[BEST_NOR][o2], np.array(datasets[BEST_NOR][o2]['response'])\n",
        "  total = modeling_with_various_features(X_tr, y_tr, X_ts, y_ts, final_columns, 'random')\n",
        "  if len(total[0]['best_classifiers']) > 0:\n",
        "    rf['->'.join([data_labels[o1], data_labels[o2]])] = total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5VHzacs9nWu"
      },
      "source": [
        "rf.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7co5aLEL9ncr"
      },
      "source": [
        "logistic.update(dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P0D43HWGMWz"
      },
      "source": [
        "logistic.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF6HZWytdhKA"
      },
      "source": [
        "##dt['MDA_MVAC->AMC'][0]['scores_tr_val']\n",
        "def draw_box_plots(tag, score_dict, best_cls_names):\n",
        "  tags = tag.split('->')\n",
        "  m_lb = np.array([f'M_{tags[0]}_NR'] * MAX_ROW)\n",
        "  m_lb[np.array(datasets[BEST_NOR][data_labels.index(tags[0])]['response'])[over_idx_dict[tags[0]]] == 1.0] = f'M_{tags[0]}_R'\n",
        "  v_lb = np.array([f'V_{tags[1]}_NR'] * len(datasets[BEST_NOR][data_labels.index(tags[1])]['response']))\n",
        "  v_lb[datasets[BEST_NOR][data_labels.index(tags[1])]['response'] == 1.0] = f'V_{tags[1]}_R'\n",
        "  #print(np.concatenate([m_lb, v_lb]))\n",
        "  for b in best_cls_names:\n",
        "    #print(np.concatenate(score_dict[b]))\n",
        "    fe.plot_box(np.concatenate(score_dict[b]), 'y', np.concatenate([m_lb, v_lb]), tag + ' + ' + b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1y27g-Wgnha"
      },
      "source": [
        "draw_box_plots('MDA_MVAC->AMC', dt['MDA_MVAC->AMC'][0]['scores_tr_val'], dt['MDA_MVAC->AMC'][0]['best_classifiers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS5QEprNjw7w"
      },
      "source": [
        "for l in logistic.keys():\n",
        "  draw_box_plots(l, logistic[l][0]['scores_tr_val'], logistic[l][0]['best_classifiers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjCbdcZ4niF8"
      },
      "source": [
        "for k, v in logistic.items():\n",
        "  for ii in k.split('->'):\n",
        "    idx = data_labels.index(ii)\n",
        "    y_str = np.array(['NR'] * len(datasets[BEST_NOR][idx]))\n",
        "    y_str[datasets[BEST_NOR][idx]['response'] == 1.0] = 'R'\n",
        "\n",
        "    cls_names = list(v[1].index)\n",
        "    for cls in cls_names:\n",
        "      dd = datasets[BEST_NOR][idx][final_columns[cls]]\n",
        "      dd.index = y_str\n",
        "      cl.plot_cluster_heatmap(dd.T, title='+'.join([ii, cls]), figsize=(10, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiMt04Bho4yt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}